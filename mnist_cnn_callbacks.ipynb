{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\meakh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "from tensorflow.keras import Sequential,Model\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "log_dir = os.path.join('logs')\n",
    "tensor_board = tf.keras.callbacks.TensorBoard(log_dir = log_dir,histogram_freq=1, profile_batch = 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 - 6s - loss: 0.0447 - accuracy: 0.9864 - val_loss: 0.0307 - val_accuracy: 0.9901\n",
      "Epoch 2/15\n",
      "60000/60000 - 6s - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.0311 - val_accuracy: 0.9902\n",
      "Epoch 3/15\n",
      "60000/60000 - 6s - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0331 - val_accuracy: 0.9892\n",
      "Epoch 4/15\n",
      "60000/60000 - 6s - loss: 0.0362 - accuracy: 0.9886 - val_loss: 0.0285 - val_accuracy: 0.9911\n",
      "Epoch 5/15\n",
      "60000/60000 - 6s - loss: 0.0312 - accuracy: 0.9902 - val_loss: 0.0301 - val_accuracy: 0.9909\n",
      "Epoch 6/15\n",
      "60000/60000 - 6s - loss: 0.0306 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9909\n",
      "Epoch 7/15\n",
      "60000/60000 - 6s - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.0288 - val_accuracy: 0.9919\n",
      "Epoch 8/15\n",
      "60000/60000 - 6s - loss: 0.0255 - accuracy: 0.9919 - val_loss: 0.0319 - val_accuracy: 0.9908\n",
      "Epoch 9/15\n",
      "60000/60000 - 6s - loss: 0.0242 - accuracy: 0.9915 - val_loss: 0.0337 - val_accuracy: 0.9911\n",
      "Epoch 10/15\n",
      "60000/60000 - 6s - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.0304 - val_accuracy: 0.9915\n",
      "Epoch 11/15\n",
      "60000/60000 - 6s - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0289 - val_accuracy: 0.9914\n",
      "Epoch 12/15\n",
      "60000/60000 - 6s - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0305 - val_accuracy: 0.9919\n",
      "Epoch 13/15\n",
      "60000/60000 - 6s - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.0300 - val_accuracy: 0.9911\n",
      "Epoch 14/15\n",
      "60000/60000 - 6s - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0312 - val_accuracy: 0.9916\n",
      "Epoch 15/15\n",
      "60000/60000 - 6s - loss: 0.0203 - accuracy: 0.9932 - val_loss: 0.0283 - val_accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=15, verbose=2,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x184439a70f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 - 6s - loss: 0.0140 - accuracy: 0.9951 - val_loss: 0.0332 - val_accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=1, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Model weights are saved at the end of every epoch, if it's the best seen\n",
    "# so far.\n",
    "model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0107 - accuracy: 0.9964 - val_loss: 0.0357 - val_accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "path = 'abcd'\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = path,\n",
    "                                                     monitor = 'val_accuracy',\n",
    "                                                     save_weights_only = True,\n",
    "                                                     mode = 'max',\n",
    "                                                     save_best_only = True)\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=1, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1abd0909c50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0338 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\"\n",
    "tensor_board = tf.keras.callbacks.TensorBoard(log_dir = log_dir,histogram_freq=1, profile_batch = 100000000)\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=1, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.0353 - val_accuracy: 0.9920\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0361 - val_accuracy: 0.9929\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0348 - val_accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=15, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0363 - val_accuracy: 0.9929\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0367 - val_accuracy: 0.9938\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0370 - val_accuracy: 0.9929\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0403 - val_accuracy: 0.9932\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0351 - val_accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00061"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def scheduler(epoch,lr):\n",
    "        return lr*math.exp(-0.1)\n",
    "    \n",
    "lr_call = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=15, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board, lr_call, early_stop])\n",
    "round(model.optimizer.lr.numpy(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 2/15\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9988\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 6.359935634802584e-18.\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 3/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9986\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.271987143504129e-18.\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 4/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.5439743283672885e-19.\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 5/15\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 5.087948656734577e-20.\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 6/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0175897054975213e-20.\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 7/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.035179346371557e-21.\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 8/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0703586927431143e-22.\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 10/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.14071738548623e-23.\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 11/15\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.628143527584344e-23.\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 12/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.2562871813864324e-24.\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 13/15\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.5125745205450455e-25.\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 14/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9990\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.3025148646659638e-25.\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0376 - val_accuracy: 0.9934\n",
      "Epoch 15/15\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.605029729331928e-26.\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.2,\n",
    "                              patience=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training.log')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=15, verbose=1,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[tensor_board, reduce_lr, csv_logger])\n",
    "round(model.optimizer.lr.numpy(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
